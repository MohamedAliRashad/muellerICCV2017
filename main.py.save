
import HALNet_torch
import torch
from torch.autograd import Variable
import torch.optim as optim
import io_data
import torch.nn.functional as F
import shutil
import visualize
import numpy as np
import resnet
from torchviz import make_dot

VERBOSE = True
LEARNING_RATE = 0.01
MOMENTUM = 0.5
LOG_INTERVAL = 100
NUM_EPOCHS = 100

#losses = []
#a= range(100)
#with open('/home/paulo/training_NN') as f:
 #   for line in f:
  #      losses.append(float(line.split('\t')[1][5:-1]))

#plt.plot(losses[0:100])
#plt.ylabel('Losses')
#plt.show()

train_loader = io_data.get_HALNet_trainloader(VERBOSE)
valid_loader = io_data.get_HALNet_validloader(VERBOSE)

print("Loading RESNet50...")
resnet50 = resnet.resnet50(pretrained=True)
#visualize.save_graph_pytorch_model(resnet50,
 #                                  model_input_shape=(1, 3, 227, 227),
  #                                 folder='', modelname='resnet50')
print("Done loading RESNet50")

if VERBOSE:
    print("Building HALNet network...")
halnet = HALNet_torch.HALNet()
if VERBOSE:
    print("Done building HALNet network")
#visualize.save_graph_pytorch_model(halnet,
 #                                  model_input_shape=(1, 4, 320, 240),
  #                                 folder='', modelname='halnet')

# initialize HALNet with RESNet50
print("Initializaing HALNet with RESNet50...")
# initialize level 1
# initialize conv1
resnet_weight = resnet50.conv1.weight.data
float_tensor = np.random.normal(np.mean(resnet_weight.numpy()),
                                np.std(resnet_weight.numpy()),
                                (resnet_weight.shape[0],
                                 1, resnet_weight.shape[2],
                                 resnet_weight.shape[2]))
resnet_weight_numpy = resnet_weight.numpy()
resnet_weight = np.concatenate((resnet_weight_numpy, float_tensor), axis=1)
resnet_weight = torch.FloatTensor(resnet_weight)
halnet.conv1[0]._parameters['weight'].data.copy_(resnet_weight)
# initialize level 2
# initialize res2a
resnet_weight = resnet50.layer1[0].conv1.weight.data
halnet.res2a.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[0].conv2.weight.data
halnet.res2a.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[0].conv3.weight.data
halnet.res2a.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[0].downsample[0].weight.data
halnet.res2a.left_res[0]._parameters['weight'].data.copy_(resnet_weight)
# initialize res2b
resnet_weight = resnet50.layer1[1].conv1.weight.data
halnet.res2b.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[1].conv2.weight.data
halnet.res2b.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[1].conv3.weight.data
halnet.res2b.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
# initialize res2c
resnet_weight = resnet50.layer1[2].conv1.weight.data
halnet.res2c.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[2].conv2.weight.data
halnet.res2c.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer1[2].conv3.weight.data
halnet.res2c.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
# initialize res3a
resnet_weight = resnet50.layer2[0].conv1.weight.data
halnet.res3a.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[0].conv2.weight.data
halnet.res3a.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[0].conv3.weight.data
halnet.res3a.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[0].downsample[0].weight.data
halnet.res3a.left_res[0]._parameters['weight'].data.copy_(resnet_weight)
# initialize res3b
resnet_weight = resnet50.layer2[1].conv1.weight.data
halnet.res3b.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[1].conv2.weight.data
halnet.res3b.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[1].conv3.weight.data
halnet.res3b.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
# initialize res3c
resnet_weight = resnet50.layer2[2].conv1.weight.data
halnet.res3c.right_res[0][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[2].conv2.weight.data
halnet.res3c.right_res[2][0]._parameters['weight'].data.copy_(resnet_weight)
resnet_weight = resnet50.layer2[2].conv3.weight.data
halnet.res3c.right_res[4][0]._parameters['weight'].data.copy_(resnet_weight)
print("Done initializaing HALNet with RESNet50")
del resnet50

def load_checkpoint(filename):
    halnet = HALNet_torch.HALNet()
    halnet.load_state_dict(torch.load(filename))
    return halnet

def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, 'model_best.pth.tar')

def validate(model, valid_loader):
    # switch to evaluate mode
    model.eval()

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.l1_loss(output, target)

def pixel_stdev(norm_heatmap):
    num_pixels = norm_heatmap.size
    mean_norm_heatmap = np.mean(norm_heatmap)
    stdev_norm_heatmap = np.std(norm_heatmap)
    lower_bound = mean_norm_heatmap - stdev_norm_heatmap
    upper_bound = mean_norm_heatmap + stdev_norm_heatmap
    pixel_count_lower = np.where(norm_heatmap >= lower_bound)
    pixel_count_upper = np.where(norm_heatmap <= upper_bound)
    pixel_count_mask = pixel_count_lower and pixel_count_upper
    return np.sqrt(norm_heatmap[pixel_count_mask].size)

def print_target_info(target):
    if len(target.shape) == 4:
        target = target[0, :, :, :]
    target = io_data.convert_torch_dataoutput_to_canonical(target.data.numpy()[0])
    norm_target = io_data.normalize_output(target)
    # get joint inference from max of heatmap
    max_heatmap = np.unravel_index(np.argmax(norm_target, axis=None), norm_target.shape)
    print("Heamap max: " + str(max_heatmap))
    # data_image = visualize.add_squares_for_joint_in_color_space(data_image, max_heatmap, color=[0, 50, 0])
    # sample from heatmap
    heatmap_sample_flat_ix = np.random.choice(range(len(norm_target.flatten())), 1, p=norm_target.flatten())
    heatmap_sample_uv = np.unravel_index(heatmap_sample_flat_ix, norm_target.shape)
    heatmap_mean = np.mean(norm_target)
    heatmap_stdev = np.std(norm_target)
    print("Heatmap mean: " + str(heatmap_mean))
    print("Heatmap stdev: " + str(heatmap_stdev))
    print("Heatmap pixel standard deviation: " + str(pixel_stdev(norm_target)))
    heatmap_sample_uv = (int(heatmap_sample_uv[0]), int(heatmap_sample_uv[1]))
    print("Heatmap sample: " + str(heatmap_sample_uv))

def train(model, optimizer, train_loader, epoch):
    model.train()
    losses = []
    best_dist_losses = []
    best_loss = 1e10
    best_dist_loss = 1e10
    for batch_idx, (data, target) in enumerate(train_loader):
        is_best = False
        data, target = Variable(data), Variable(target)
        #print_target_info(target)
        #visualize.show_halnet_data_as_image(data)
        optimizer.zero_grad()
        output = model(data)
        max_output = np.unravel_index(np.argmax(
            io_data.convert_torch_dataoutput_to_canonical(
                output.data.numpy()[0]), axis=None), output.shape)
        max_target = np.unravel_index(np.argmax(
            io_data.convert_torch_dataoutput_to_canonical(
                target.data.numpy()[0]), axis=None), target.shape)
        dist_loss = np.sqrt(np.power((max_output[0] - max_target[0]), 2) +
                            np.power((max_output[1] - max_target[1]), 2))
        if dist_loss < best_dist_loss:
            best_dist_loss = dist_loss
            print("\tThis is a best dist loss found so far: " + str(best_dist_loss))
        best_dist_losses.append(dist_loss)
        #print_target_info(output)
        loss = F.l1_loss(output, target)
        loss_float = loss.data[0]
        losses.append(loss_float)
        if loss_float < best_loss:
            best_loss = loss_float
            print("\tThis is a best loss found so far: " + str(loss_float))
            is_best = True
        loss.backward()
        optimizer.step()
        if batch_idx % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                       100. * batch_idx / len(train_loader), loss.data[0]))
            # save a checkpoint
            print("Saving a checkpoint...")
            save_checkpoint({
                'best_dist_losses': best_dist_losses,
                'losses': losses,
                'epoch': epoch + 1,
                'is_best': is_best,
                'state_dict': model.state_dict(),
                'best_prec1': 1,
                'optimizer': optimizer.state_dict(),
            }, is_best)

optimizer = optim.SGD(halnet.parameters(),
                          lr=LEARNING_RATE,
                          momentum=MOMENTUM)

for epoch in range(NUM_EPOCHS):
    train(halnet, optimizer, train_loader, epoch)
